# âœ… ML Pipeline Deployment Checklist

## Pre-Deployment Verification

### 1. Files Created âœ…
- [x] `docker-compose.yaml` - Updated with Spark and ML worker
- [x] `Dockerfile.ml` - ML worker container definition
- [x] `requirements.txt` - Python ML dependencies
- [x] `scripts/ml_classification.py` - Classification models
- [x] `scripts/ml_forecasting.py` - Prophet forecasting
- [x] `scripts/spark_regression.py` - Spark MLlib regression
- [x] `scripts/ml_pipeline.py` - Orchestrator
- [x] `dags/nyc_pipeline_dag.py` - Updated with ML tasks
- [x] `models/` directory - For trained models
- [x] `reports/` directory - For visualizations
- [x] Documentation files

## Deployment Steps

### Step 1: Start Docker Desktop â³
```bash
# Open Docker Desktop application
# Wait for "Docker Desktop is running" status
```
- [ ] Docker Desktop is running

### Step 2: Build ML Worker Image â³
```bash
cd /Users/sandilyachimalamarri/data-pipeline-project
docker compose build ml-worker
```
**Expected**: Building process completes successfully (~5-10 minutes first time)
- [ ] ML worker image built successfully
- [ ] No build errors in output

### Step 3: Start All Services â³
```bash
docker compose up -d
```
**Expected**: All containers start and become healthy
- [ ] PostgreSQL running
- [ ] Redis running
- [ ] Airflow services running (webserver, scheduler, worker)
- [ ] Metabase running
- [ ] **ML Worker running** âœ¨

### Step 4: Verify Services â³
```bash
docker compose ps
```
**Expected**: All services show "healthy" or "running" status
- [ ] All containers are up
- [ ] No containers in "restarting" or "unhealthy" state

### Step 5: Test ML Environment â³
```bash
# Test Python libraries
docker compose exec ml-worker python -c "import sklearn, xgboost, prophet, pyspark; print('âœ… All ML libraries working!')"
```
**Expected**: `âœ… All ML libraries working!`
- [ ] All ML libraries import successfully
- [ ] No ModuleNotFoundError

### Step 6: Verify Data Availability â³
```bash
# Check data
docker compose exec postgres psql -U airflow -d airflow -c "SELECT COUNT(*) FROM nyc_311_bronx_full_year;"
```
**Expected**: Returns count > 1,000,000 (from your full-year data fetch)
- [ ] Data table exists
- [ ] Sufficient data for ML training (>10,000 records minimum)

### Step 7: Run Classification Models â³
```bash
docker compose exec ml-worker python /app/scripts/ml_classification.py
```
**Expected Output**:
```
ğŸ¤– NYC 311 COMPLAINT TYPE CLASSIFICATION PIPELINE
ğŸ” Loading 50000 records from database...
âœ… Loaded 50000 records
...
1ï¸âƒ£  Training Logistic Regression...
   âœ… Logistic Regression trained
2ï¸âƒ£  Training Random Forest...
   âœ… Random Forest trained
3ï¸âƒ£  Training XGBoost...
   âœ… XGBoost trained
...
ğŸ‰ CLASSIFICATION PIPELINE COMPLETED SUCCESSFULLY!
```
- [ ] Classification models trained
- [ ] Accuracy > 60%
- [ ] Models saved to `/app/models/`

### Step 8: Run Time Series Forecasting â³
```bash
docker compose exec ml-worker python /app/scripts/ml_forecasting.py
```
**Expected Output**:
```
ğŸ”® NYC 311 TIME SERIES FORECASTING PIPELINE (PROPHET)
ğŸ” Loading time series data...
âœ… Loaded XXX days of data
...
ğŸ¤– Training Prophet model...
   âœ… Model trained successfully
ğŸ”® Generating 30-day forecast...
   âœ… Forecast generated
...
ğŸ‰ FORECASTING PIPELINE COMPLETED SUCCESSFULLY!
```
- [ ] Prophet model trained
- [ ] Forecast generated
- [ ] Plots saved to `/app/reports/`
- [ ] MAPE < 30%

### Step 9: Run Spark Regression â³
```bash
docker compose exec ml-worker python /app/scripts/spark_regression.py
```
**Expected Output**:
```
âš¡ NYC 311 SPARK MLlib LINEAR REGRESSION PIPELINE
ğŸš€ Creating Spark session...
âœ… Spark session created
...
ğŸ¤– Training Spark MLlib Linear Regression...
   âœ… Model trained
ğŸ“Š MODEL PERFORMANCE:
   Root Mean Squared Error (RMSE): XX.XX
   RÂ² Score: 0.XXXX
...
ğŸ‰ SPARK MLlib PIPELINE COMPLETED SUCCESSFULLY!
```
- [ ] Spark session created
- [ ] Model trained successfully
- [ ] RÂ² score > 0.30

### Step 10: Run Complete Pipeline â³
```bash
docker compose exec ml-worker python /app/scripts/ml_pipeline.py
```
**Expected**: All 3 ML tasks complete sequentially
- [ ] Classification completes
- [ ] Forecasting completes
- [ ] Spark regression completes
- [ ] Exit code 0 (success)

### Step 11: Verify Airflow Integration â³
```bash
# Access Airflow UI
open http://localhost:8080
# Login: airflow / airflow
```
- [ ] Airflow UI accessible
- [ ] `nyc_data_pipeline` DAG visible
- [ ] DAG shows new ML tasks
- [ ] Can trigger DAG manually

### Step 12: Trigger Full DAG Run â³
In Airflow UI:
1. Find `nyc_data_pipeline`
2. Unpause the DAG (toggle to green)
3. Click "Play" button to trigger
4. Watch tasks execute

**Expected Flow**:
```
fetch_311_data â†’ fetch_weather_data â†’ join_311_and_weather â†’ ml_pipeline_orchestrator
```
- [ ] Data tasks complete
- [ ] ML pipeline task starts
- [ ] All tasks show green (success)

### Step 13: Check Generated Outputs â³
```bash
# Check models
ls -lh models/

# Check reports
ls -lh reports/

# Check database
docker compose exec postgres psql -U airflow -d airflow -c "SELECT * FROM ml_forecast_results LIMIT 5;"
```
- [ ] Model files exist (.pkl files)
- [ ] Report files exist (.png files)
- [ ] Forecast table populated

### Step 14: Access Metabase â³
```bash
open http://localhost:3000
```
- [ ] Metabase accessible
- [ ] Can connect to PostgreSQL
- [ ] Can query `ml_forecast_results` table

## Post-Deployment Verification

### Smoke Tests âœ…
```bash
# Quick health check script
docker compose ps | grep -E "(healthy|running)" | wc -l
# Should show 7+ services

docker compose exec ml-worker python -c "import os; print('Models:', len([f for f in os.listdir('/app/models') if f.endswith('.pkl')]))"
# Should show multiple model files

docker compose exec postgres psql -U airflow -d airflow -c "SELECT COUNT(*) FROM ml_forecast_results;"
# Should return > 0 rows
```
- [ ] All smoke tests pass

### Performance Check âœ…
```bash
# Check resource usage
docker stats --no-stream ml-worker
```
- [ ] Memory usage < 4GB
- [ ] CPU usage reasonable
- [ ] No container restarts

## Troubleshooting Guide

### Issue: Build fails with dependency errors
**Solution**:
```bash
docker compose build --no-cache ml-worker
```

### Issue: Containers won't start
**Solution**:
```bash
docker compose down
docker compose up -d
docker compose logs ml-worker
```

### Issue: Python import errors
**Solution**:
```bash
docker compose exec ml-worker pip install --upgrade scikit-learn xgboost prophet pyspark
```

### Issue: Database connection fails
**Solution**:
```bash
docker compose restart postgres
# Wait 30 seconds
docker compose exec postgres psql -U airflow -d airflow -c "SELECT 1"
```

## Success Criteria ğŸ¯

Your deployment is successful when ALL of these are true:
- âœ… All Docker containers running healthy
- âœ… ML classification accuracy > 60%
- âœ… Prophet forecast MAPE < 30%
- âœ… Spark regression RÂ² > 0.30
- âœ… Models saved in `/models/` directory
- âœ… Forecast plots in `/reports/` directory
- âœ… Airflow DAG runs end-to-end successfully
- âœ… Metabase can query forecast results

## Final Validation Command
```bash
# Run this to validate everything
cd /Users/sandilyachimalamarri/data-pipeline-project
./validate_ml_setup.sh
```

## Need Help?

1. **Check logs**: `docker compose logs ml-worker --tail=50`
2. **Review docs**: `ML_SETUP_GUIDE.md`
3. **Quick ref**: `COMMANDS_QUICK_REFERENCE.md`
4. **Summary**: `ML_IMPLEMENTATION_SUMMARY.md`

---

## Sign-off

When all items are checked:
- Deployment Date: _______________
- Deployed By: _______________
- All Tests Passing: [ ] Yes / [ ] No
- Production Ready: [ ] Yes / [ ] No

**ğŸ‰ Congratulations! Your ML pipeline is now live!** ğŸš€

